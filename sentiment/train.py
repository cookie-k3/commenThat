import urllib.request
import pandas as pd
import numpy as np
import pickle
import os
from konlpy.tag import Mecab
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Dropout, Conv1D, MaxPooling1D, LSTM, Dense, Activation
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

# âœ… mecabrc í™˜ê²½ë³€ìˆ˜ ì„¤ì •
os.environ["MECABRC"] = "/opt/homebrew/etc/mecabrc"

# ğŸ“ ê²½ë¡œ ì„¤ì •
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_DIR = os.path.join(BASE_DIR, "../models")
os.makedirs(MODEL_DIR, exist_ok=True)

# 1. ë°ì´í„° ë‹¤ìš´ë¡œë“œ
url = "https://raw.githubusercontent.com/bab2min/corpus/master/sentiment/naver_shopping.txt"
urllib.request.urlretrieve(url, filename="ratings_total.txt")
data = pd.read_table("ratings_total.txt", names=["ratings", "reviews"])

# 2. ë ˆì´ë¸” ìƒì„±
data['label'] = np.select([data.ratings > 3], [1], default=0)
data.drop_duplicates(subset=['reviews'], inplace=True)
data['reviews'] = data['reviews'].replace('', np.nan)  # FutureWarning í•´ê²°
data.dropna(how='any', inplace=True)

# 3. í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• 
train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)

# 4. í˜•íƒœì†Œ ë¶„ì„ + ë¶ˆìš©ì–´ ì œê±°
try:
    possible_paths = [
        '/opt/homebrew/lib/mecab/dic/mecab-ko-dic',
        '/usr/local/lib/mecab/dic/mecab-ko-dic',
        '/usr/local/Cellar/mecab-ko-dic/2.1.1-20180720/lib/mecab/dic/mecab-ko-dic',
        '/opt/homebrew/Cellar/mecab-ko-dic/2.1.1-20180720/lib/mecab/dic/mecab-ko-dic'
    ]

    for path in possible_paths:
        if os.path.exists(path):
            mecab = Mecab(dicpath=path)
            print(f"ì‚¬ìš© ì¤‘ì¸ MeCab ì‚¬ì „ ê²½ë¡œ: {path}")
            break
    else:
        try:
            mecab = Mecab()
            print("ê¸°ë³¸ MeCab ì„¤ì • ì‚¬ìš©")
        except:
            import subprocess
            result = subprocess.run(['find', '/', '-name', 'mecab-ko-dic', '-type', 'd'],
                                    stdout=subprocess.PIPE, stderr=subprocess.DEVNULL, text=True)
            paths = result.stdout.strip().split('\n')
            if paths and paths[0]:
                mecab = Mecab(dicpath=paths[0])
                print(f"ì°¾ì€ MeCab ì‚¬ì „ ê²½ë¡œ: {paths[0]}")
            else:
                raise RuntimeError("MeCab-ko-dicë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì§ì ‘ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
except Exception as e:
    print(f"MeCab ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
    print("\nì•„ë˜ ëª…ë ¹ì–´ë¡œ MeCabê³¼ í•œêµ­ì–´ ì‚¬ì „ì„ ì„¤ì¹˜í•´ë³´ì„¸ìš”:")
    print("brew install mecab mecab-ko-dic")
    print("pip install mecab-python3")
    raise

stopwords = ['ë„', 'ëŠ”', 'ë‹¤', 'ì˜', 'ê°€', 'ì´', 'ì€', 'í•œ', 'ì—', 'í•˜', 'ê³ ', 'ì„', 'ë¥¼', 'ì¸', 'ë“¯', 'ê³¼', 'ì™€', 'ë„¤', 'ë“¤', 'ë“¯', 'ì§€', 'ì„', 'ê²Œ']

def tokenize(text):
    return [t for t in mecab.morphs(text) if t not in stopwords]

train_data['tokenized'] = train_data['reviews'].apply(tokenize)
test_data['tokenized'] = test_data['reviews'].apply(tokenize)

# 5. í† í¬ë‚˜ì´ì € í•™ìŠµ ë° ì €ì¥
X_train = train_data['tokenized'].values
X_test = test_data['tokenized'].values
y_train = train_data['label'].values
y_test = test_data['label'].values

tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)

# í¬ê·€ ë‹¨ì–´ ì œì™¸
total_cnt = len(tokenizer.word_index)
rare_cnt = 0
threshold = 3
for key, value in tokenizer.word_counts.items():
    if value < threshold:
        rare_cnt += 1
vocab_size = total_cnt - rare_cnt + 2

# í† í¬ë‚˜ì´ì € ë‹¤ì‹œ ìƒì„± + ì €ì¥
tokenizer = Tokenizer(vocab_size, oov_token="OOV")
tokenizer.fit_on_texts(X_train)
with open(os.path.join(MODEL_DIR, "tokenizer.pickle"), "wb") as f:
    pickle.dump(tokenizer, f)

# ì •ìˆ˜ ì¸ì½”ë”© & íŒ¨ë”©
X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

max_len = 80
X_train = pad_sequences(X_train, maxlen=max_len)
X_test = pad_sequences(X_test, maxlen=max_len)

# 6. ëª¨ë¸ êµ¬ì„±
model = Sequential()
model.add(Embedding(vocab_size, 100))
model.add(Dropout(0.5))
model.add(Conv1D(64, 5, activation='relu'))
model.add(MaxPooling1D(pool_size=4))
model.add(LSTM(250, return_sequences=True))
model.add(LSTM(250, return_sequences=True))
model.add(LSTM(250))
model.add(Dense(1))
model.add(Activation('sigmoid'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

es = EarlyStopping(monitor='val_loss', mode='min', patience=4)
mc = ModelCheckpoint(os.path.join(MODEL_DIR, 'best_model.h5'), monitor='val_accuracy', mode='max', save_best_only=True)

model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test), callbacks=[es, mc])